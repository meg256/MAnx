import pandas as pd
from snakemake.utils import min_version

##### set minimum snakemake version #####
min_version("8.8.0")

##### setup report #####
configfile: "config/config.yaml"

RESULTS_DIR = config["results"]
report: "report/workflow.rst"
#report: "report/assemblyqc.html"

##### setup singularity ####
# this container defines the underlying OS for each job when using the workflow
# with --use-singularity
container: "docker://continuumio/miniconda3:4.4.10"

#### samples ####
SAMPLES = pd.read_csv(config["samples"]).set_index(["sample", "species"], drop=False)
ECOLI_SAMS = SAMPLES.loc[SAMPLES["species"]=="Ecoli"]

def get_species_samples(wildcards):
    subs = SAMPLES.loc[SAMPLES["species"]==wildcards.species]
    return subs["sample"]

def get_read_paths(wildcards):
    return {"FW": SAMPLES.loc[(wildcards.sample, wildcards.species), ['R1']].dropna(), "RV": SAMPLES.loc[(wildcards.sample, wildcards.species), ['R2']].dropna()}

##### target rules #####

aggregated_input = ["report/filtered_assembly_report.tsv",
                    expand("results/quast/filtered_assembly_report.tsv"),#, species=set(SAMPLES["species"])), #quast
                    expand("results/filtered_assembly/skesa/{species}/{sample}.filtered.fasta", zip, species=SAMPLES['species'], sample=SAMPLES['sample']), #filter contigs
                    expand("results/assembly/skesa/{species}/{sample}.fa", zip, species=SAMPLES['species'], sample=SAMPLES['sample']), #assembly
                    expand("results/trimmed/BBtools/{species}/{sample}_1.trimmed.fastq.gz", zip, species=SAMPLES['species'], sample=SAMPLES['sample']), #trim
                    expand("results/trimmed/BBtools/{species}/{sample}_2.trimmed.fastq.gz", zip, species=SAMPLES['species'], sample=SAMPLES['sample']),
                    expand("results/rawreads/{species}/{sample}_1.fastq.gz", zip, species=SAMPLES['species'], sample=SAMPLES['sample']), #copy raw reads
                    expand("results/rawreads/{species}/{sample}_2.fastq.gz", zip, species=SAMPLES['species'], sample=SAMPLES['sample'])
]

if "Ecoli" in list(SAMPLES["species"]):
    aggregated_input.append("results/mlst/Ecoli_mlst.tsv") #checkpoints.ecoli_pymlst.get(**wildcards).output[0]
    aggregated_input.append("report/Ecoli_mlst.tsv")

rule all:
    input:
        aggregated_input,

#rule input_check
# placeholder: assume this checks input files exist and are valid paired fastq.gz, output location is possible, etc

rule copy_raw_reads:
    input:
        unpack(get_read_paths),
    output:
        FWOUT="results/rawreads/{species}/{sample}_1.fastq.gz", 
        RVOUT="results/rawreads/{species}/{sample}_2.fastq.gz"
    shell:
        """
        [[ -f {input.FW} ]] && cp {input.FW} {output.FWOUT}
        [[ -f {input.RV} ]] && cp {input.RV} {output.RVOUT}
        """
rule trim:
    input:
        FW="results/rawreads/{species}/{sample}_1.fastq.gz", 
        RV="results/rawreads/{species}/{sample}_2.fastq.gz"
    output:
        FWOUT="results/trimmed/BBtools/{species}/{sample}_1.trimmed.fastq.gz",
        RVOUT="results/trimmed/BBtools/{species}/{sample}_2.trimmed.fastq.gz"
    params:
        numThreads = config['numThreads'],
    conda:
        "envs/bbtools.yaml"
    shell: "bbduk.sh -Xmx10g t={params.numThreads} in1={input.FW} in2={input.RV} out1={output.FWOUT} out2={output.RVOUT} ref=adapters,phix ktrim=r k=31 mink=11 hdist=1 tpe tbo"

rule assemble:
    input: 
        FW="results/trimmed/BBtools/{species}/{sample}_1.trimmed.fastq.gz",
        RV="results/trimmed/BBtools/{species}/{sample}_2.trimmed.fastq.gz"
    output:
        "results/assembly/skesa/{species}/{sample}.fa",
    params:
        numThreads = config['numThreads'],
    conda:
        "envs/skesa.yaml"
    shell: "skesa --reads {input.FW},{input.RV} --vector_percent 1 --cores {params.numThreads} > {output}"

rule filter_contigs:
    input:
        "results/assembly/skesa/{species}/{sample}.fa",
    output:
        "results/filtered_assembly/skesa/{species}/{sample}.filtered.fasta"
    shell:
        "python3 workflow/scripts/filter_contigs.py -i {input} -o {output} -c 20 -l 500"

rule quast:
    input:
        expand("results/filtered_assembly/skesa/{species}/{sample}.filtered.fasta", zip, species=SAMPLES['species'], sample=SAMPLES['sample']),# zip, species=set(SAMPLES["species"]), sample=get_species_samples,
    output:
        quastdir = temp(directory("results/quast/report")),
        quastfile = "results/quast/filtered_assembly_report.tsv",
        quastrep = report("report/filtered_assembly_report.tsv", caption="report/filtered_assembly_report.rst", category="quast"),
    conda:
        "envs/quast.yaml",
    shell:
        """
        quast  -o  {output.quastdir} {input}
        mv {output.quastdir}/transposed_report.tsv {output.quastfile}
        cp {output.quastfile} {output.quastrep}       
        """

checkpoint ecoli_pymlst:
    input:
        expand("results/filtered_assembly/skesa/Ecoli/{sample}.filtered.fasta", sample=ECOLI_SAMS["sample"]),
    output:
        mlstout = "results/mlst/Ecoli_mlst.tsv",
        reportout = report("report/Ecoli_mlst.tsv", caption="report/Ecoli_mlst.rst", category="mlst"),
    params:
        database = "testrun/Escherichia.db" #can use function to get species-specific database OR name databases by species
        #database = check_database(config["pymlst"]["database"]),
        #min_id = check_min_id(config["pymlst"]["min_id"]),
    conda:
        "envs/pymlst.yaml"
    shell: 
        """
        claMLST search {params.database} {input} -o {output.mlstout}
        cp {output.mlstout} {output.reportout}
        """


# move individual html files around for final report directory
#rule generate_html_hierarchy:
#    output: report(directory("test"), caption="report/caption.rst", htmlindex="test.html")
#    shell:
#        """
#        # mimic writing of an HTML hierarchy
#        mkdir test
#        cp template.html test/test.html
#        mkdir test/js
#        echo \"alert('test')\" > test/js/test.js
#        """



###############################################################
#UNITS = pd.read_csv(config['units'])
#datasets = list(UNITS['dataset'])
#unitdict = dict(zip(UNITS.dataset,UNITS.numreads))
#print(unitdict)

#rule downsample:
#    input:
#        unpack(get_read_paths),
#    output:
#        #FWOUT = expand(f"results/reads/lowcov/{{sp}}/{{sample}}_1.fastq.gz", zip, sp=SPECIES, sample=SAMS),
#        #RVOUT = expand(f"results/reads/lowcov/{{sp}}/{{sample}}_2.fastq.gz", zip, sp=SPECIES, sample=SAMS),
##        expand(f"results/reads/lowcov/{{sp}}/{{sample}}_1.fastq.gz, zip, sp=samples["species"], sample=samples["samplename"]),
#        expand(f"results/reads/lowcov/{{sp}}/{{sample}}_2.fastq.gz, zip, sp=samples["species"], sample=samples["samplename"]),
#        #expand(expand("results/reads/{{d}}/{sp}/{sample}_1.fastq.gz", zip, sp=SAMPLES["sample"], sample=SAMPLES["species"]), d=datasets, read=[["1", "2"]]),
#    params:
#        numsams = len(SAMPLES),
#        numreads = 100000, #get_readnum
#    container:
#        config["singularity_input"],    
#    shell:
#        """
#        SEED=echo $((1 + $RANDOM % {params.numsams}))
##        FWLN=$(gunzip -c {input.FW} | awk 'END {print NR/4}');
##       RVLN=$(gunzip -c {input.RV} | awk 'END {print NR/4}');
#        FWFRAC=$(python3 -c "print('{:.2f}'.format({numreads} / $RVLN))")
#       RVFRAC=$(python3 -c "print('{:.2f}'.format({numreads} / $RVLN))")
#        seqtk sample -s $SEED {input.FW} $FWFRAC > {output.FWOUT};
#        seqtk sample -s $SEED {input.RV} $RVFRAC > {output.RVOUT};
#        """


# PYTHONIC INPUT SPECIFYING EXPAND WITH ZIP 
#https://stackoverflow.com/questions/75532295/snakemake-expand-zip-some-wildcards-and-expand-full-the-other
# for sample, species in zip(BASES, CONTRASTS):
#     for trimtool in TRIMCOND:
#         for d in DATASETS:
#             FW = "results/trimmed/{d}/{trimtool}/{species}/{sample}_1.trimmed.fastq.gz"
#             RV = "results/trimmed/{d}/{trimtool}/{species}/{sample}_2.trimmed.fastq.gz"
#             ALL_TRIMMED.append(file)
# rule all:
#    input: ALL_TRIMMED

# OR: NESTED FOR LOOP
# https://stackoverflow.com/questions/67999553/how-to-make-a-nested-loop-in-snakemake
# for trimtool in config["trim"]: #["trimmomatic", "bbduk"]
#     #import toolrulename
#     rule:
#         name: f"trim_{trimtool}"
#         input: unpack(get_read_paths),
#         output: f"path/to/{tool}/outputfile"
#         shell: f"{tool} {{input}} > {{output}}"
    
#for i,d in enumerate(datasets):
#    rulename = f"input_{i}"
#    rule rulename:
#        input:
#            FW = get_R1,
#            RV = get_R2,
#        output:
#            FWOUT = expand(f"results/reads/{d}/{{sp}}/{{sample}}_1.fastq.gz", zip, sp=SPECIES, #sample=SAMS),
#            RVOUT = expand(f"results/reads/{d}/{{sp}}/{{sample}}_2.fastq.gz", zip, sp=SPECIES, #sample=SAMS),
#        params:
#            numsams = len(SAMPLES),
#            numreads = UNITS.loc[d, ["numreads"]].dropna()
#        container:
#            config["singularity_input"],    
#        shell:
##            """
#            SEED=echo $((1 + $RANDOM % {params.numsams}))
#            FWLN=$(gunzip -c {input.FW} | awk 'END {print NR/4}');
#            RVLN=$(gunzip -c {input.RV} | awk 'END {print NR/4}');
#            FWFRAC=$(python3 -c "print('{:.2f}'.format({numreads} / $RVLN))")
#            RVFRAC=$(python3 -c "print('{:.2f}'.format({numreads} / $RVLN))")
#            seqtk sample -s $SEED {input.FW} $FWFRAC > {output.FWOUT};
#            seqtk sample -s $SEED {input.RV} $RVFRAC > {output.RVOUT};
#            """


# wildcard_constraints:
#    # Constraints maybe not needed but in my opinion better to set them
#    sample='|'.join([re.escape(x) for x in samples['samplename']]),
#    filepath='|'.join([re.escape(x) for x in samples.filepath]),
#    species='|'.join([re.escape(x) for x in samples.species]),


# #could use a lookup function to get species or genome ref?
# #def _get_fastx(wildcards, column_name = samples_column):
# #   return samples.loc[wildcards.sample, [column_name]].dropna()[0]

##### Modules #####
#include: "rules/get_isolates.smk"
#include: "rules/trim.smk"
#include: "rules/assemble.smk"
#include: "rules/postassembly.smk"
#include: "rules/stats.smk"

        #"reports/package_versions.tsv",
        #"reports/assemblyqc.html", 
        #"results/quast/combinedquast.tsv",
        #"results/checkm/combinedcheckm.tsv",
        #expand("assembly/{d}/{trimcond}/{assem}/{species}/{sample}.fasta"), #all fastas from all assemblers from all trim conditions from all downsampled datasets
        #expand("results/trimmed/{d}/trimmomatic/{sp}/{sample}_2.trimmed.fastq.gz", zip, d=datasets, sp=SPECIES, sample=SAMS), #all trimmed reads in all downsampled datasets
        #expand("results/trimmed/{d}/trimmomatic/{sp}/{sample}_1.trimmed.fastq.gz", zip, d=datasets, sp=SPECIES, sample=SAMS),


#samples = pd.read_csv(config["samples"], dtype=str).set_index(["sample"], drop=False)
#SPECIES = list(SAMPLES["species"])
#SAMS = list(SAMPLES["sample"])

#SAMPLES = pd.read_csv(config["samples"], sep="\t").set_index(["species", "sample"], drop=False).sort_index()


#def get_readnum(dataset):
#    return UNITS.loc[wildcards.dataset, ["numreads"]].dropna()
